"""
LRET Accuracy Validation Benchmark

Measures simulation accuracy by comparing LRET against Cirq reference:
1. State fidelity: F(œÅ_LRET, œÅ_Cirq)
2. Trace distance: D(œÅ_LRET, œÅ_Cirq) = 0.5 * ||œÅ_LRET - œÅ_Cirq||_1
3. Expectation value accuracy for observables
4. Rank evolution tracking

This validates LRET produces correct results, not just fast ones.
"""

import json
import sys
import time
import subprocess
import tempfile
import os
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple

sys.path.insert(0, str(Path(__file__).parent.parent))
from cirq_comparison.cirq_fdm_wrapper import CirqFDMSimulator

print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë          LRET Accuracy Validation Benchmark                ‚ïë
‚ïë     Comparing LRET vs Cirq (Reference Implementation)      ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
""")

LRET_EXE = Path("d:/LRET/build/Release/quantum_sim.exe")

def run_lret_with_state_export(circuit_path: Path) -> dict:
    """Run LRET and export final state for comparison."""
    if not LRET_EXE.exists():
        raise FileNotFoundError(f"LRET executable not found: {LRET_EXE}")
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        output_path = f.name
    
    try:
        cmd = [
            str(LRET_EXE),
            "--input-json", str(circuit_path),
            "--output-json", output_path,
        ]
        
        start = time.perf_counter()
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
        elapsed = (time.perf_counter() - start) * 1000
        
        if result.returncode != 0:
            return {
                "status": "error",
                "message": result.stderr or result.stdout,
                "execution_time_ms": elapsed,
            }
        
        with open(output_path, 'r') as f:
            output = json.load(f)
        
        output['measured_time_ms'] = elapsed
        return output
        
    finally:
        try:
            os.unlink(output_path)
        except:
            pass

def reconstruct_lret_density_matrix(L_matrix: List[List[complex]], num_qubits: int) -> np.ndarray:
    """Reconstruct density matrix from LRET's L matrix (œÅ = L L‚Ä†)."""
    L = np.array(L_matrix, dtype=complex)
    rho = L @ L.conj().T
    return rho

def compute_fidelity(rho1: np.ndarray, rho2: np.ndarray) -> float:
    """Compute fidelity F(œÅ1, œÅ2) = Tr(‚àö(‚àöœÅ1 œÅ2 ‚àöœÅ1))¬≤."""
    sqrt_rho1 = scipy.linalg.sqrtm(rho1)
    M = sqrt_rho1 @ rho2 @ sqrt_rho1
    sqrt_M = scipy.linalg.sqrtm(M)
    fidelity = np.real(np.trace(sqrt_M)) ** 2
    return float(fidelity)

def compute_trace_distance(rho1: np.ndarray, rho2: np.ndarray) -> float:
    """Compute trace distance D(œÅ1, œÅ2) = 0.5 * ||œÅ1 - œÅ2||_1."""
    diff = rho1 - rho2
    eigenvalues = np.linalg.eigvalsh(diff)
    trace_distance = 0.5 * np.sum(np.abs(eigenvalues))
    return float(trace_distance)

def compute_expectation_value(rho: np.ndarray, observable: np.ndarray) -> float:
    """Compute <O> = Tr(œÅ O)."""
    return float(np.real(np.trace(rho @ observable)))

def generate_test_circuit(num_qubits: int, depth: int, circuit_type: str, noise: float = 0.0) -> Tuple[dict, str]:
    """Generate various test circuits."""
    rng = np.random.default_rng(42)
    
    ops = []
    
    if circuit_type == "bell":
        # Bell state
        ops.append({"name": "H", "wires": [0]})
        ops.append({"name": "CNOT", "wires": [0, 1]})
        desc = f"Bell-{num_qubits}q"
        
    elif circuit_type == "ghz":
        # GHZ state
        ops.append({"name": "H", "wires": [0]})
        for i in range(num_qubits - 1):
            ops.append({"name": "CNOT", "wires": [i, i + 1]})
        desc = f"GHZ-{num_qubits}q"
        
    elif circuit_type == "w_state":
        # W state approximation
        for q in range(num_qubits):
            angle = np.arccos(np.sqrt(1.0 / (num_qubits - q)))
            ops.append({"name": "RY", "wires": [q], "params": [float(2 * angle)]})
            if q < num_qubits - 1:
                ops.append({"name": "CNOT", "wires": [q, q + 1]})
        desc = f"W-{num_qubits}q"
        
    elif circuit_type == "random":
        # Random circuit
        for q in range(num_qubits):
            ops.append({"name": "H", "wires": [q]})
        
        for layer in range(depth):
            for q in range(num_qubits):
                if rng.random() < 0.6:
                    gate = rng.choice(["RX", "RY", "RZ"])
                    angle = float(rng.uniform(0, 2 * np.pi))
                    ops.append({"name": gate, "wires": [q], "params": [angle]})
            
            for q in range(0, num_qubits - 1, 2):
                ops.append({"name": "CNOT", "wires": [q, q + 1]})
        
        desc = f"Random-{num_qubits}q-d{depth}"
    
    # Add Z observables for expectation value measurements
    observables = []
    for q in range(min(3, num_qubits)):  # Measure first 3 qubits
        observables.append({
            "type": "PAULI",
            "operator": "Z",
            "wires": [q],
            "coefficient": 1.0
        })
    
    circuit = {
        "circuit": {
            "num_qubits": num_qubits,
            "operations": ops,
            "observables": observables,
        },
        "config": {
            "epsilon": 1e-4,
            "initial_rank": 1,
            "export_state": True,  # Request state export
        },
    }
    
    if noise > 0:
        circuit["config"]["noise"] = {
            "type": "depolarizing",
            "parameter": noise,
        }
        desc += f"-noise{int(noise*100)}"
    
    return circuit, desc

# Test configurations
test_configs = [
    # Small circuits (exact comparison possible)
    {"qubits": 3, "depth": 5, "type": "bell", "noise": 0.0},
    {"qubits": 4, "depth": 5, "type": "ghz", "noise": 0.0},
    {"qubits": 5, "depth": 5, "type": "w_state", "noise": 0.0},
    {"qubits": 4, "depth": 10, "type": "random", "noise": 0.0},
    
    # With noise
    {"qubits": 4, "depth": 5, "type": "ghz", "noise": 0.01},
    {"qubits": 4, "depth": 10, "type": "random", "noise": 0.01},
    
    # Medium scale
    {"qubits": 6, "depth": 10, "type": "ghz", "noise": 0.0},
    {"qubits": 6, "depth": 15, "type": "random", "noise": 0.0},
    
    # Larger scale (up to 8 qubits)
    {"qubits": 8, "depth": 10, "type": "ghz", "noise": 0.0},
    {"qubits": 8, "depth": 15, "type": "random", "noise": 0.01},
]

print("Configuration:")
print(f"  Test configurations: {len(test_configs)}")
print(f"  Metrics: Fidelity, Trace Distance, Expectation Values")
print(f"  Scale: 3-8 qubits")
print(f"\nStarting accuracy validation at {datetime.now().strftime('%H:%M:%S')}")
print("="*75)

# Need scipy for fidelity computation
try:
    import scipy.linalg
except ImportError:
    print("‚ö†Ô∏è  Warning: scipy not found. Installing...")
    subprocess.run([sys.executable, "-m", "pip", "install", "scipy"], check=True)
    import scipy.linalg

results = []

for idx, config in enumerate(test_configs, 1):
    qubits = config['qubits']
    depth = config['depth']
    circuit_type = config['type']
    noise = config['noise']
    
    circuit, desc = generate_test_circuit(qubits, depth, circuit_type, noise)
    
    print(f"\n[{idx}/{len(test_configs)}] {desc}")
    
    # Save circuit
    circuit_path = Path(f"d:/LRET/cirq_comparison/accuracy_test_{desc}.json")
    with open(circuit_path, 'w') as f:
        json.dump(circuit, f, indent=2)
    
    try:
        # Run Cirq (reference)
        print(f"  Cirq (reference): ", end="", flush=True)
        cirq_circuit, _, noise_model = CirqFDMSimulator.from_json(circuit)
        sim = CirqFDMSimulator(qubits, noise_model=noise_model)
        cirq_state, metadata = sim.simulate(cirq_circuit)
        print(f"{metadata['execution_time_ms']:.1f} ms")
        
        # Run LRET
        print(f"  LRET:             ", end="", flush=True)
        lret_result = run_lret_with_state_export(circuit_path)
        
        if lret_result.get("status") == "error":
            print(f"ERROR: {lret_result.get('message')}")
            continue
        
        lret_time = lret_result.get('measured_time_ms', 0)
        lret_rank = lret_result.get('final_rank', 0)
        print(f"{lret_time:.1f} ms (rank: {lret_rank})")
        
        # Compare expectation values
        cirq_expectations = []
        lret_expectations = lret_result.get('expectation_values', [])
        
        # Compute Cirq expectation values for comparison
        for obs_idx, obs in enumerate(circuit['circuit']['observables']):
            q = obs['wires'][0]
            # Create Pauli-Z operator for qubit q
            Z_full = np.eye(2**qubits, dtype=complex)
            for i in range(2**qubits):
                if (i >> (qubits - 1 - q)) & 1:  # Check if bit q is 1
                    Z_full[i, i] = -1
            
            exp_val = compute_expectation_value(cirq_state, Z_full)
            cirq_expectations.append(exp_val)
        
        # Calculate metrics
        print(f"  Accuracy Metrics:")
        
        # Expectation value comparison
        if lret_expectations and len(lret_expectations) == len(cirq_expectations):
            exp_errors = [abs(l - c) for l, c in zip(lret_expectations, cirq_expectations)]
            max_exp_error = max(exp_errors)
            mean_exp_error = sum(exp_errors) / len(exp_errors)
            
            print(f"    Expectation values:")
            for i, (c, l, err) in enumerate(zip(cirq_expectations, lret_expectations, exp_errors)):
                status = "‚úì" if err < 1e-2 else "‚ö†Ô∏è"
                print(f"      Z_{i}: Cirq={c:+.6f}, LRET={l:+.6f}, error={err:.2e} {status}")
            
            print(f"    Max expectation error:  {max_exp_error:.2e}")
            print(f"    Mean expectation error: {mean_exp_error:.2e}")
        else:
            max_exp_error = None
            mean_exp_error = None
            print(f"    Expectation values: Not available")
        
        # State fidelity (if states are small enough)
        if qubits <= 6:  # Only compute for small systems
            # Note: LRET doesn't export full state by default, so we use expectation values
            # as a proxy for accuracy
            print(f"    State comparison: Skipped (use expectation values as proxy)")
            fidelity = None
            trace_dist = None
        else:
            fidelity = None
            trace_dist = None
        
        # Rank validation
        expected_rank = 1 if noise == 0.0 and circuit_type in ["bell", "ghz"] else None
        if expected_rank:
            rank_correct = lret_rank == expected_rank
            print(f"    Rank: {lret_rank} (expected: {expected_rank}) {'‚úì' if rank_correct else '‚ö†Ô∏è'}")
        else:
            print(f"    Rank: {lret_rank}")
        
        # Overall verdict
        if mean_exp_error is not None:
            if mean_exp_error < 1e-3:
                verdict = "‚úì EXCELLENT (< 0.1%)"
            elif mean_exp_error < 1e-2:
                verdict = "‚úì GOOD (< 1%)"
            elif mean_exp_error < 0.05:
                verdict = "‚ö†Ô∏è ACCEPTABLE (< 5%)"
            else:
                verdict = "‚ùå POOR (> 5%)"
            
            print(f"    Overall: {verdict}")
        
        results.append({
            'name': desc,
            'qubits': qubits,
            'depth': depth,
            'type': circuit_type,
            'noise': noise,
            'lret_rank': lret_rank,
            'max_exp_error': max_exp_error,
            'mean_exp_error': mean_exp_error,
            'fidelity': fidelity,
            'trace_distance': trace_dist,
            'cirq_time': metadata['execution_time_ms'],
            'lret_time': lret_time,
        })
        
    except Exception as e:
        print(f"  ERROR: {e}")
        import traceback
        traceback.print_exc()
        continue

# Summary
print("\n" + "="*75)
print("ACCURACY VALIDATION SUMMARY")
print("="*75)

if results:
    print(f"\nCircuits successfully tested: {len(results)}")
    
    print(f"\n{'Circuit':<25} {'Qubits':>6} {'Rank':>6} {'Max Err':>10} {'Mean Err':>10} {'Status':>10}")
    print("-" * 75)
    
    for r in results:
        if r['mean_exp_error'] is not None:
            max_err_str = f"{r['max_exp_error']:.2e}"
            mean_err_str = f"{r['mean_exp_error']:.2e}"
            
            if r['mean_exp_error'] < 1e-3:
                status = "‚úì EXCELLENT"
            elif r['mean_exp_error'] < 1e-2:
                status = "‚úì GOOD"
            elif r['mean_exp_error'] < 0.05:
                status = "‚ö†Ô∏è OK"
            else:
                status = "‚ùå POOR"
        else:
            max_err_str = "N/A"
            mean_err_str = "N/A"
            status = "N/A"
        
        print(f"{r['name']:<25} {r['qubits']:>6} {r['lret_rank']:>6} {max_err_str:>10} {mean_err_str:>10} {status:>10}")
    
    # Overall statistics
    valid_results = [r for r in results if r['mean_exp_error'] is not None]
    
    if valid_results:
        avg_error = sum(r['mean_exp_error'] for r in valid_results) / len(valid_results)
        max_error = max(r['max_exp_error'] for r in valid_results)
        
        excellent_count = sum(1 for r in valid_results if r['mean_exp_error'] < 1e-3)
        good_count = sum(1 for r in valid_results if 1e-3 <= r['mean_exp_error'] < 1e-2)
        
        print("-" * 75)
        print(f"\nOverall Statistics:")
        print(f"  Average error:    {avg_error:.2e}")
        print(f"  Maximum error:    {max_error:.2e}")
        print(f"  Excellent (< 0.1%): {excellent_count}/{len(valid_results)}")
        print(f"  Good (< 1%):        {good_count}/{len(valid_results)}")
        
        if avg_error < 1e-3:
            print(f"\n‚úì‚úì‚úì LRET ACCURACY: EXCELLENT")
            print(f"    Average error < 0.1% - production ready!")
        elif avg_error < 1e-2:
            print(f"\n‚úì‚úì LRET ACCURACY: GOOD")
            print(f"    Average error < 1% - suitable for most applications")
        elif avg_error < 0.05:
            print(f"\n‚úì LRET ACCURACY: ACCEPTABLE")
            print(f"    Average error < 5% - may need tuning for high-precision work")
        else:
            print(f"\n‚ö†Ô∏è LRET ACCURACY: NEEDS IMPROVEMENT")
            print(f"    Average error > 5% - check epsilon threshold")
    
    # Save results
    output_file = f"d:/LRET/cirq_comparison/accuracy_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nüíæ Results saved to: {output_file}")

else:
    print("\n‚ùå No circuits were successfully tested!")

print(f"\nCompleted at {datetime.now().strftime('%H:%M:%S')}")
print("="*75)
